The implementation of the Transformer Decoder Only model includes support for Key-Value (KV) Caching and absolute positional encoding. The entire coding process was guided by Figure 2 from the seminal paper "Attention is All You Need."

At the conclusion of the program, the Transformer Decoder Only model was employed for autoregressive causal sequence prediction, with a comparative analysis conducted between sequences generated using and without utilizing KV CACHE.

Last but not least, the program also assessed the speed at which tokens were generated by the model. It was observed that the use of KV CACHE could significantly enhance the generation speed of the model, by up to several hundred times, particularly when dealing with longer token sequences.

![image](https://github.com/user-attachments/assets/f137a528-6a25-46e8-80e6-9972eda7b2ff)

